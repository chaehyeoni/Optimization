최적화 알고리즘
=============

###재고량 예측하기
    해당 파일 : 1.py, 1-1.py
    
    과거의 판매량과 과거의 구매량을 가지고 최적의 구매량을 예측하기 위해서 재고 주기를 알아냄
    미래에 구매해야하는 양을 예측함으로써 최적의 구매량으로 이어질 수 있음을 확인함
    최고로 많이 보유해야하는 양에서 평균적인 재고량의 차이를 구해 보유하고 있어야만 물건이 입고될 때까지 재고를 유지하도록
    
    안전재고량 산출법
    일일 최고 판매량 x 최대 리드타임 - 일일 평균 판매량 x 평균 리드타임
    (리드 타임 : 발주에서 입고까지 걸리는 시간)
    
    적정 재고의 산출법
    최대 리드타임 - 평균 리드타임 = 지연 가능한 최대 리드타임
    최대 판매량 - 평균 판매량이 일간 안전 재고량
    적정 재고는 구매 주기 패턴을 파악하는 것이 좋음
    
    미래 구매량 구하는 방법
    금주 재고량 = 전주의 재고 + 금주의 구매량 - 금주의 판매량
    재고주기 = 금주 재고량 / 과거 4주 판매 평균
    재고주기 = (전주의 재고 + 금주의 구매량 - 금주의 판매량)/과거 4주 판매 평균
    
    위에 항을 정리
    (전주의 재고 + 금주의 구매량 - 금주의 판매량) = 재고주기 x 과거 4주 판매 평균
    금주의 구매량 = 재고주기 x 과거 4주 판매 평균 - 전주의 재고 + 금주의 판매량
    
    판매량에 대한 표준편차를 이용하면 안정 재고량을 판단하는데 도움
    
### 선형회귀
    데이터를 시각적으로 확인하기 위해 scatter 차트를 만듦
    시각적으로 봐야지 어떻게 처리해야할지 알 수 있기 때문
    
    이를 위해 데이터를 바탕으로 선형 모델을 작성함
    기울기 구하는 방법 (2.py)
    기울기에 알맞을 것 같은 모든 경우의 수를 구해 오차가 최소가 되는 값을 찾아서 그것이 최적의 기울기인지 판단
    주의점 : 분모가 0이 안되게 만드는 것, 분모가 음수에서 양수까지 가기 때문에 음수일 때는 체크 할 필요가 없음
    이를 구현할 때 실행착오 :
    너무 동떨어진 것이 나옴 (기울기를 너무 크게 구해서)
    그렇기 떄문에 추가적으로 더 나눠주었다 (기울기를 보정해줌)
    결국 y절편이 필요함을 알게 됨
    이를 구하기 위해 최소 제곱법을 사용하는 것이 좋겠다고 판단함 
    
    (그래서 2-1.py를 만듬)
    최소 제곱법 구하는 방법
    a = (x-x평균)(y-y평균)의 합 / (x-x평균)의 제곱의 합
    반복문을 통해서 최소 제곱법을 직접 구현해봄
    최소 제곱법을 사용한 것은 단항식이었음 (속성이 하나)
    그렇기 떄문에 polyfit을 사용하여 다차방정식에 대한 최적화를 풀기 위해 사용함
    
    (그래서 2-2.py를 만듬)
    
### 보스턴 집값 예측하기
    (3.py)
    데이터 자체를 보면서 샘플링 데이터와 검증용 데이터로 분리했다 (이때 처음 시도)
    분리한 데이터들을 scatter_matrix를 사용하여 모든 데이터의 관계를 시각적으로 차트로 보여줌
    
    (3-1.py)
    다중회귀 분석을 함
    절편과 계수들을 찾아낸다음 테스트를 해봄
    실제값과 예측값을 계산하기 위해 수식을 만들어줌
    오차의 값들의 차가 가장 적어지는 데이터가 무엇인지 찾아봄
    이 다중회귀 분석을 익숙하게 하기 위해서 캘리포니아 집값을 알아봄 (3-2.py)
    
    (4.py) 
    r문법을 사용함
    
    결국 최적화라는 것은 다음 선택을 도와준다는 것을 깨달음
    최적화라는 것은 기본적인 예측에 있음
    예측 - 실행 - 결과 확인 - 평가 - 보정 순으로 일어남
    지금까지 한 것들은 과거를 바탕으로 미래에 어떻게 일어날지에 대해서 계산한것  (피드백 사이클)
    회귀분석과 다른 내용인 식별에 대해서 해봄
    식별은 과거의 정보를 근거로 현 상황을 식별을 목표로 하는 것
    보통 공장을 운영 할 때 사용할 수도 있음 
    (-> 이것이 자동으로 작동되면 인공지능으로 넘어가는 것)
    킥보드 같은 서비스를 사용할 때 식별을 사용함
    -> 어느 곳에 두어야지 사람들이 많이 탈지에 대해 식별해야했음
    식별을 하기 위해서는 clustering, classification 방식 이 있음
    clustering은 뭉쳐놓고 판단하는 것 (후분석을 위해 사용함)
    라벨이 들어가기 때문에 바로 예측이 가능함
    kmeans 알고리즘을 처음 사용함 (k는 클러스터의 개수가 몇개인지 구하는 의미)
    1부터 n까지의 클러스를 만들 수 있음
    너무 적거나 너무 많으면 제대로 된 구분을 할 수 없기 때문이다.
    -> 눈대중으로 판단하거나 수치를 계산해서 구하는 두가지 방법이 있었음
    (4-2.py)
    그래프 중 꺾이는 부분이 최적인 부분임
    
    knn은 어떠한 평가 지표를 사용하여 진행하는 것
    나랑 가까이 있는 k를 보고 자신이 무엇인지 판단하는 것
    (예를 들어 내 주변에 학생만 있으면 내가 학생이고, 내 주변에 선생님만 있다면 나는 선생님일 가능성이 큼)
    라벨별로 색깔을 구분해서 차트를 구현함
    -> neighbors = 2 라는 것은 내 옆의 2개의 값을 보고 내 값을 유추한다는 뜻
    
    이를 익숙하게 하기 위해서 서울에 있는 좌표는 무슨 동인지 구별하는 것을 해봄
    방이동과 오금동의 경계에 있는 좌표는 오차가 발생할 확률이 크기 때문에 위험요소가 있었음
    
    지도 좌표를 가지고 직접 예측도 해봄
    
    knn에서 k의 값을 정하는 것은 휴리스틱에 가까움
    최적의 k를 구하기 위해서 k의 갯수를 계속 늘리면서 어디의 값이 제일 높은지를 찾아냄
    (차트를 통해 시각적으로 확인함)
    
    아웃라이어(평균에서 크게 벗어난) 데이터들이 있는 경우 올바른 식별이 불가능했음
    따라서 특정 레이블로 분류하기 위해서는 임계점이 필요했음
    이를 해결하기 위해 로지스틱 회귀를 사용함
    임계점을 조정하는 방식을 사용함
    대학교 합격 유,무 예척, 재난 상황에서 생존자 예측, 스팸메일 유,무, 암 유,무 등에 응용할 수 있음
    
    로지스틱 회귀법에는
    log odds, sigmoid function, loss function(손실함수)가 중요함 -> 공부하기
    공부를 일정값안에 안한다고 대학에 합격하지 못하는 것이 아니기 때문에 sigmoid 함수를 사용하면 일정값 안에 이쁘게 그래프로 나타낼 수 있음
    (확률에 대한 문제로 치환이 됨)
    loss 함수는 오차 범위를 줄이기 위해 사용함
    
    (6.py) -> 로지스틱 회귀 처음 사용
    데이터가 분류성 데이터이기 떄문에 색을 입혀 쉽게 특징을 찾아냄
    데이터가 너무 많으면 다 보이지 않는다는 단점이 있음
    
    레이블이 없는 데이터는 names에 값을 넣어 라벨을 임의로 부여해 줬었음 
    
    카테고리 데이터들이 많기 때문에 숫자 데이터로 치환해야 했었음
    하지만 데이터가 누락된 차이가 필요했기 때문에 없는 값이 있는 행을 제거해줌 
    
    날짜 정보가 문자열로 되어있었기 떄문에 컴퓨터가 처리할 수 있는 값으로 불러와서 처리했었음
    (8.py)
    one hot encoding을 사용함
    특정 속성의 영향을 줄이기 위해서 사용함 사용성은 까다롭지만, 이를 이용하면 데이터의 영향도를 줄일 수 있음
    
    데이터를 분석하는 방법
    목표값을 설정한다 (y값)
    카테고리성 데이터가 있는지 판단하고 만약 2개 이상 잇다면 knn을, 2개만 있다면 로지스틱을 활용
    
    선형회귀를 평가하는 방법에는 r2 score과 차트로 예상 값을 비교하는 두가지 방법이 있음
    r2 score이 무엇인지 공부하기***
    이를 바탕으로 bank_marketing에 관한 데이터를 사용하여 마케팅을 실행한이후에 고객이
    해당 상품에 가입을 했는지 파악하는 활동을 해봄
    
    
### ㅇㄹㅇ